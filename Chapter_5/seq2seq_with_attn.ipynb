{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import nltk\nimport numpy as np\nimport re\nimport shutil\nimport tensorflow as tf\nimport os\nimport unicodedata\nimport zipfile\n\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef clean_up_logs(data_dir):\n    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n    if os.path.exists(checkpoint_dir):\n        shutil.rmtree(checkpoint_dir, ignore_errors=True)\n        os.makedirs(checkpoint_dir)\n    return checkpoint_dir\n\n\ndef preprocess_sentence(sent):\n    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \n        if unicodedata.category(c) != \"Mn\"])\n    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n    sent = re.sub(r\"\\s+\", \" \", sent)\n    sent = sent.lower()\n    return sent\n\n\ndef download_and_read(url, num_sent_pairs=30000):\n    local_file = url.split('/')[-1]\n    if not os.path.exists(local_file):\n        os.system(\"wget -O {:s} {:s}\".format(local_file, url))\n        with zipfile.ZipFile(local_file, \"r\") as zip_ref:\n            zip_ref.extractall(\".\")\n    local_file = os.path.join(\".\", \"fra.txt\")\n    en_sents, fr_sents_in, fr_sents_out = [], [], []\n    with open(local_file, \"r\") as fin:\n        for i, line in enumerate(fin):\n            en_sent, fr_sent = line.strip().split('\\t')\n            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n            fr_sent = preprocess_sentence(fr_sent)\n            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n            en_sents.append(en_sent)\n            fr_sents_in.append(fr_sent_in)\n            fr_sents_out.append(fr_sent_out)\n            if i >= num_sent_pairs - 1:\n                break\n    return en_sents, fr_sents_in, fr_sents_out\n\n\nclass BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, num_units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(num_units)\n        self.W2 = tf.keras.layers.Dense(num_units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        # query is the decoder state at time step j\n        # query.shape: (batch_size, num_units)\n        # values are encoder states at every timestep i\n        # values.shape: (batch_size, num_timesteps, num_units)\n\n        # add time axis to query: (batch_size, 1, num_units)\n        query_with_time_axis = tf.expand_dims(query, axis=1)\n        # compute score:\n        score = self.V(tf.keras.activations.tanh(\n            self.W1(values) + self.W2(query_with_time_axis)))\n        # compute softmax\n        alignment = tf.nn.softmax(score, axis=1)\n        # compute attended output\n        context = tf.reduce_sum(\n            tf.linalg.matmul(\n                tf.linalg.matrix_transpose(alignment),\n                values\n            ), axis=1\n        )\n        context = tf.expand_dims(context, axis=1)\n        return context, alignment\n\n\nclass LuongAttention(tf.keras.layers.Layer):\n    def __init__(self, num_units):\n        super(LuongAttention, self).__init__()\n        self.W = tf.keras.layers.Dense(num_units)\n\n    def call(self, query, values):\n        # add time axis to query\n        query_with_time_axis = tf.expand_dims(query, axis=1)\n        # compute score\n        score = tf.linalg.matmul(\n            query_with_time_axis, self.W(values), transpose_b=True)\n        # compute softmax\n        alignment = tf.nn.softmax(score, axis=2)\n        # compute attended output\n        context = tf.matmul(alignment, values)\n        return context, alignment\n\n\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, num_timesteps, \n            embedding_dim, encoder_dim, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.encoder_dim = encoder_dim\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            encoder_dim, return_sequences=True, return_state=True)\n\n    def call(self, x, state):\n        x = self.embedding(x)\n        x, state = self.rnn(x, initial_state=state)\n        return x, state\n\n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.encoder_dim))\n\n\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, num_timesteps,\n            decoder_dim, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.decoder_dim = decoder_dim\n\n        # self.attention = LuongAttention(embedding_dim)\n        self.attention = BahdanauAttention(embedding_dim)\n\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            decoder_dim, return_sequences=True, return_state=True)\n\n        self.Wc = tf.keras.layers.Dense(decoder_dim, activation=\"tanh\")\n        self.Ws = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, x, state, encoder_out):\n        x = self.embedding(x)\n        context, alignment = self.attention(x, encoder_out)\n        x = tf.expand_dims(\n                tf.concat([\n                    x, tf.squeeze(context, axis=1)\n                ], axis=1), \n            axis=1)\n        x, state = self.rnn(x, state)\n        x = self.Wc(x)\n        x = self.Ws(x)\n        return x, state, alignment\n\n\ndef loss_fn(ytrue, ypred):\n    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n    mask = tf.cast(mask, dtype=tf.int64)\n    loss = scce(ytrue, ypred, sample_weight=mask)\n    return loss\n\n\n@tf.function\ndef train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n    with tf.GradientTape() as tape:\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n        decoder_state = encoder_state\n\n        loss = 0\n        for t in range(decoder_out.shape[1]):\n            decoder_in_t = decoder_in[:, t]\n            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n                decoder_state, encoder_out)\n            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss / decoder_out.shape[1]\n\n\ndef predict(encoder, decoder, batch_size, \n        sents_en, data_en, sents_fr_out, \n        word2idx_fr, idx2word_fr):\n    random_id = np.random.choice(len(sents_en))\n    print(\"input    : \",  \" \".join(sents_en[random_id]))\n    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n\n    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n\n    encoder_state = encoder.init_state(1)\n    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n    decoder_state = encoder_state\n\n    pred_sent_fr = []\n    decoder_in = tf.expand_dims(\n        tf.constant(word2idx_fr[\"BOS\"]), axis=0)\n\n    while True:\n        decoder_pred, decoder_state, _ = decoder(\n            decoder_in, decoder_state, encoder_out)\n        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n        pred_sent_fr.append(pred_word)\n        if pred_word == \"EOS\":\n            break\n        decoder_in = tf.squeeze(decoder_pred, axis=1)\n\n    print(\"predicted: \", \" \".join(pred_sent_fr))\n\n\ndef evaluate_bleu_score(encoder, decoder, test_dataset, \n        word2idx_fr, idx2word_fr):\n\n    bleu_scores = []\n    smooth_fn = SmoothingFunction()\n\n    for encoder_in, decoder_in, decoder_out in test_dataset:\n        encoder_state = encoder.init_state(batch_size)\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n        decoder_state = encoder_state\n\n        ref_sent_ids = np.zeros_like(decoder_out)\n        hyp_sent_ids = np.zeros_like(decoder_out)\n        for t in range(decoder_out.shape[1]):\n            decoder_out_t = decoder_out[:, t]\n            decoder_in_t = decoder_in[:, t]\n            decoder_pred_t, decoder_state, _ = decoder(\n                decoder_in_t, decoder_state, encoder_out)\n            decoder_pred_t = tf.argmax(decoder_pred_t, axis=-1)\n            for b in range(decoder_pred_t.shape[0]):\n                ref_sent_ids[b, t] = decoder_out_t.numpy()[0]\n                hyp_sent_ids[b, t] = decoder_pred_t.numpy()[0][0]\n        \n        for b in range(ref_sent_ids.shape[0]):\n            ref_sent = [idx2word_fr[i] for i in ref_sent_ids[b] if i > 0]\n            hyp_sent = [idx2word_fr[i] for i in hyp_sent_ids[b] if i > 0]\n            # remove trailing EOS\n            ref_sent = ref_sent[0:-1]\n            hyp_sent = hyp_sent[0:-1]\n            bleu_score = sentence_bleu([ref_sent], hyp_sent,\n                smoothing_function=smooth_fn.method1)\n            bleu_scores.append(bleu_score)\n\n    return np.mean(np.array(bleu_scores))\n\n\n# NUM_SENT_PAIRS = 100\n# EMBEDDING_DIM = 32\n# ENCODER_DIM, DECODER_DIM = 64, 64\n# BATCH_SIZE = 8\n# NUM_EPOCHS = 3\n\nNUM_SENT_PAIRS = 30000\nEMBEDDING_DIM = 256\nENCODER_DIM, DECODER_DIM = 1024, 1024\nBATCH_SIZE = 64\nNUM_EPOCHS = 30\n\ntf.random.set_seed(42)\n\ndata_dir = \"./data\"\ncheckpoint_dir = clean_up_logs(data_dir)\n\n# Test code for attention classes\n# batch_size = BATCH_SIZE\n# num_timesteps = MAXLEN_EN\n# num_units = ENCODER_DIM\n\n# query = np.random.random(size=(batch_size, num_units))\n# values = np.random.random(size=(batch_size, num_timesteps, num_units))\n\n# # check out dimensions for Bahdanau attention\n# b_attn = BahdanauAttention(num_units)\n# context, alignments = b_attn(query, values)\n# print(\"Bahdanau: context.shape:\", context.shape, \"alignments.shape:\", alignments.shape)\n# # Bahdanau: context.shape: (64, 1024) alignments.shape: (64, 8, 1)\n\n# # check out dimensions for Luong attention\n# l_attn = LuongAttention(num_units)\n# context, alignments = l_attn(query, values)\n# print(\"Luong: context.shape:\", context.shape, \"alignments.shape:\", alignments.shape)\n# # Luong: context.shape: (64, 1024) alignments.shape: (64, 8, 1)\n# End test code for attention classes\n\n# data preparation\ndownload_url = \"http://www.manythings.org/anki/fra-eng.zip\"\nsents_en, sents_fr_in, sents_fr_out = download_and_read(\n    download_url, num_sent_pairs=NUM_SENT_PAIRS)\n\ntokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\", lower=False)\ntokenizer_en.fit_on_texts(sents_en)\ndata_en = tokenizer_en.texts_to_sequences(sents_en)\ndata_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")\n\ntokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\", lower=False)\ntokenizer_fr.fit_on_texts(sents_fr_in)\ntokenizer_fr.fit_on_texts(sents_fr_out)\ndata_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\ndata_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\ndata_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\ndata_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\n\nmaxlen_en = data_en.shape[1]\nmaxlen_fr = data_fr_out.shape[1]\nprint(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))\n\nbatch_size = BATCH_SIZE\ndataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\ndataset = dataset.shuffle(10000)\ntest_size = NUM_SENT_PAIRS // 4\ntest_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\ntrain_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)\n\nvocab_size_en = len(tokenizer_en.word_index)\nvocab_size_fr = len(tokenizer_fr.word_index)\nword2idx_en = tokenizer_en.word_index\nidx2word_en = {v:k for k, v in word2idx_en.items()}\nword2idx_fr = tokenizer_fr.word_index\nidx2word_fr = {v:k for k, v in word2idx_fr.items()}\nprint(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\n    vocab_size_en, vocab_size_fr))\n# vocab size (en): 57, vocab size (fr): 123\n\n# check encoder/decoder dimensions\nembedding_dim = EMBEDDING_DIM\nencoder_dim, decoder_dim = ENCODER_DIM, DECODER_DIM\n\nencoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\ndecoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)\n\n# # Test code for encoder and decoder with attention\n# for encoder_in, decoder_in, decoder_out in train_dataset:\n#     print(\"inputs:\", encoder_in.shape, decoder_in.shape, decoder_out.shape)\n#     encoder_state = encoder.init_state(batch_size)\n#     encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n#     decoder_state = encoder_state\n#     decoder_pred = []\n#     for t in range(decoder_out.shape[1]):\n#         decoder_in_t = decoder_in[:, t]\n#         decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n#             decoder_state, encoder_out)\n#         decoder_pred.append(decoder_pred_t.numpy())\n#     decoder_pred = tf.squeeze(np.array(decoder_pred), axis=2)\n#     break\n# print(\"encoder input          :\", encoder_in.shape)\n# print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\n# print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\n# print(\"decoder output (labels):\", decoder_out.shape)\n\n# Bahdanau:\n# encoder input          : (64, 8)\n# encoder output         : (64, 8, 1024) state: (64, 1024)\n# decoder output (logits): (8, 64, 7658) state: (64, 1024)\n# decoder output (labels): (64, 16)\n# \n# Luong:\n# encoder input          : (64, 8)\n# encoder output         : (64, 8, 1024) state: (64, 1024)\n# decoder output (logits): (8, 64, 7658) state: (64, 1024)\n# decoder output (labels): (64, 16)\n\n\noptimizer = tf.keras.optimizers.Adam()\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)\n\nnum_epochs = NUM_EPOCHS\neval_scores = []\n\nfor e in range(num_epochs):\n    encoder_state = encoder.init_state(batch_size)\n\n    for batch, data in enumerate(train_dataset):\n        encoder_in, decoder_in, decoder_out = data\n        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n        loss = train_step(\n            encoder_in, decoder_in, decoder_out, encoder_state)\n    \n    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n\n    if e % 10 == 0:\n        checkpoint.save(file_prefix=checkpoint_prefix)\n    \n    predict(encoder, decoder, batch_size, sents_en, data_en,\n        sents_fr_out, word2idx_fr, idx2word_fr)\n\n    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n    # eval_scores.append(eval_score)\n\ncheckpoint.save(file_prefix=checkpoint_prefix)"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}