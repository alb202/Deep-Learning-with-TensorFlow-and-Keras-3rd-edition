{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef clean_logs(data_dir):\n    logs_dir = os.path.join(data_dir, \"logs\")\n    shutil.rmtree(logs_dir, ignore_errors=True)\n    return logs_dir\n\n\ndef download_and_read(url):\n    local_file = url.split('/')[-1]\n    local_file = local_file.replace(\"%20\", \" \")\n    p = tf.keras.utils.get_file(local_file, url, \n        extract=True, cache_dir=\".\")\n    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n    labeled_sentences = []\n    for labeled_filename in os.listdir(local_folder):\n        if labeled_filename.endswith(\"_labelled.txt\"):\n            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n                for line in f:\n                    sentence, label = line.strip().split('\\t')\n                    labeled_sentences.append((sentence, label))\n    return labeled_sentences\n\n\nclass SentimentAnalysisModel(tf.keras.Model):\n    def __init__(self, vocab_size, max_seqlen, **kwargs):\n        super(SentimentAnalysisModel, self).__init__(**kwargs)\n        self.embedding = tf.keras.layers.Embedding(vocab_size, max_seqlen)\n        self.bilstm = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(max_seqlen)\n        )\n        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n\n    def call(self, x):\n        x = self.embedding(x)\n        x = self.bilstm(x)\n        x = self.dense(x)\n        x = self.out(x)\n        return x\n\n\n# set random seed\ntf.random.set_seed(42)\n\n# clean up log area\ndata_dir = \"./data\"\nlogs_dir = clean_logs(data_dir)\n\n# download and read data into data structures\nlabeled_sentences = download_and_read(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\")\nsentences = [s for (s, l) in labeled_sentences]\nlabels = [int(l) for (s, l) in labeled_sentences]\n\n# tokenize sentences\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(sentences)\nvocab_size = len(tokenizer.word_counts)\nprint(\"vocabulary size: {:d}\".format(vocab_size))\n\nword2idx = tokenizer.word_index\nidx2word = {v:k for (k, v) in word2idx.items()}\n\n# seq_lengths = np.array([len(s.split()) for s in sentences])\n# print([(p, np.percentile(seq_lengths, p)) for p \n#     in [75, 80, 90, 95, 99, 100]])\n# [(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\nmax_seqlen = 64\n\n# create dataset\nsentences_as_ints = tokenizer.texts_to_sequences(sentences)\nsentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    sentences_as_ints, maxlen=max_seqlen)\nlabels_as_ints = np.array(labels)\ndataset = tf.data.Dataset.from_tensor_slices(\n    (sentences_as_ints, labels_as_ints))\n\n# split into train and test\ndataset = dataset.shuffle(10000)\ntest_size = len(sentences) // 3\nval_size = (len(sentences) - test_size) // 10\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\n\nbatch_size = 64\ntrain_dataset = train_dataset.batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\ntest_dataset = test_dataset.batch(batch_size)\n\n# define model\n# vocab_size + 1 to account for PAD character\nmodel = SentimentAnalysisModel(vocab_size+1, max_seqlen)\nmodel.build(input_shape=(batch_size, max_seqlen))\nmodel.summary()\n\n# compile\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\", \n    metrics=[\"accuracy\"]\n)\n\n# train\nbest_model_file = os.path.join(data_dir, \"best_model.h5\")\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n    save_weights_only=True,\n    save_best_only=True)\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\nnum_epochs = 10\nhistory = model.fit(train_dataset, epochs=num_epochs, \n    validation_data=val_dataset,\n    callbacks=[checkpoint, tensorboard])\n\n# evaluate with test set\nbest_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\nbest_model.build(input_shape=(batch_size, max_seqlen))\nbest_model.load_weights(best_model_file)\nbest_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\", \n    metrics=[\"accuracy\"]\n)\n\ntest_loss, test_acc = best_model.evaluate(test_dataset)\nprint(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(test_loss, test_acc))\n\n# predict on batches\nlabels, predictions = [], []\nidx2word[0] = \"PAD\"\nis_first_batch = True\nfor test_batch in test_dataset:\n    inputs_b, labels_b = test_batch\n    pred_batch = best_model.predict(inputs_b)\n    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n    labels.extend([l for l in labels_b])\n    if is_first_batch:\n        for rid in range(inputs_b.shape[0]):\n            words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n            words = [w for w in words if w != \"PAD\"]\n            sentence = \" \".join(words)\n            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[rid], predictions[rid], sentence))\n        is_first_batch = False\n\nprint(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\nprint(\"confusion matrix\")\nprint(confusion_matrix(labels, predictions))\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}