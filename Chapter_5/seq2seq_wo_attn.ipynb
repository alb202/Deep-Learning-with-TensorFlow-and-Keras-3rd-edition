{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import nltk\nimport numpy as np\nimport re\nimport shutil\nimport tensorflow as tf\nimport os\nimport unicodedata\n\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef clean_up_logs(data_dir):\n    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\n    if os.path.exists(checkpoint_dir):\n        shutil.rmtree(checkpoint_dir, ignore_errors=True)\n        os.makedirs(checkpoint_dir)\n    return checkpoint_dir\n\n\ndef preprocess_sentence(sent):\n    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \n        if unicodedata.category(c) != \"Mn\"])\n    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n    sent = re.sub(r\"\\s+\", \" \", sent)\n    sent = sent.lower()\n    return sent\n\n\ndef download_and_read():\n    en_sents, fr_sents_in, fr_sents_out = [], [], []\n    local_file = os.path.join(\"datasets\", \"fra.txt\")\n    with open(local_file, \"r\") as fin:\n        for i, line in enumerate(fin):\n            en_sent, fr_sent = line.strip().split('\\t')\n            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n            fr_sent = preprocess_sentence(fr_sent)\n            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n            en_sents.append(en_sent)\n            fr_sents_in.append(fr_sent_in)\n            fr_sents_out.append(fr_sent_out)\n            if i >= num_sent_pairs - 1:\n                break\n    return en_sents, fr_sents_in, fr_sents_out\n\n\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, num_timesteps, \n            encoder_dim, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.encoder_dim = encoder_dim\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            encoder_dim, return_sequences=False, return_state=True)\n\n    def call(self, x, state):\n        x = self.embedding(x)\n        x, state = self.rnn(x, initial_state=state)\n        return x, state\n\n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.encoder_dim))\n\n\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, num_timesteps,\n            decoder_dim, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.decoder_dim = decoder_dim\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            decoder_dim, return_sequences=True, return_state=True)\n        self.dense = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, x, state):\n        x = self.embedding(x)\n        x, state = self.rnn(x, state)\n        x = self.dense(x)\n        return x, state\n\n\ndef loss_fn(ytrue, ypred):\n    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n    mask = tf.cast(mask, dtype=tf.int64)\n    loss = scce(ytrue, ypred, sample_weight=mask)\n    return loss\n\n\n@tf.function\ndef train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n    with tf.GradientTape() as tape:\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n        decoder_state = encoder_state\n        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n        loss = loss_fn(decoder_out, decoder_pred)\n    \n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss\n\n\ndef predict(encoder, decoder, batch_size, \n        sents_en, data_en, sents_fr_out, \n        word2idx_fr, idx2word_fr):\n    random_id = np.random.choice(len(sents_en))\n    print(\"input    : \",  \" \".join(sents_en[random_id]))\n    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n\n    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n\n    encoder_state = encoder.init_state(1)\n    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n    decoder_state = encoder_state\n\n    decoder_in = tf.expand_dims(\n        tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\n    pred_sent_fr = []\n    while True:\n        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n        pred_sent_fr.append(pred_word)\n        if pred_word == \"EOS\":\n            break\n        decoder_in = decoder_pred\n    \n    print(\"predicted: \", \" \".join(pred_sent_fr))\n\n\ndef evaluate_bleu_score(encoder, decoder, test_dataset, \n        word2idx_fr, idx2word_fr):\n\n    bleu_scores = []\n    smooth_fn = SmoothingFunction()\n    for encoder_in, decoder_in, decoder_out in test_dataset:\n        encoder_state = encoder.init_state(batch_size)\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n        decoder_state = encoder_state\n        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n\n        # compute argmax\n        decoder_out = decoder_out.numpy()\n        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n\n        for i in range(decoder_out.shape[0]):\n            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\n            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\n            # remove trailing EOS\n            ref_sent = ref_sent[0:-1]\n            hyp_sent = hyp_sent[0:-1]\n            bleu_score = sentence_bleu([ref_sent], hyp_sent, \n                smoothing_function=smooth_fn.method1)\n            bleu_scores.append(bleu_score)\n\n    return np.mean(np.array(bleu_scores))\n\n\nNUM_SENT_PAIRS = 30000\nEMBEDDING_DIM = 256\nENCODER_DIM, DECODER_DIM = 1024, 1024\nBATCH_SIZE = 64\nNUM_EPOCHS = 30\n\ntf.random.set_seed(42)\n\ndata_dir = \"./data\"\ncheckpoint_dir = clean_up_logs(data_dir)\n\n# data preparation\ndownload_url = \"http://www.manythings.org/anki/fra-eng.zip\"\nsents_en, sents_fr_in, sents_fr_out = download_and_read()\n\ntokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\", lower=False)\ntokenizer_en.fit_on_texts(sents_en)\ndata_en = tokenizer_en.texts_to_sequences(sents_en)\ndata_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding=\"post\")\n\ntokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\", lower=False)\ntokenizer_fr.fit_on_texts(sents_fr_in)\ntokenizer_fr.fit_on_texts(sents_fr_out)\ndata_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\ndata_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in, padding=\"post\")\ndata_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\ndata_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out, padding=\"post\")\n\nvocab_size_en = len(tokenizer_en.word_index)\nvocab_size_fr = len(tokenizer_fr.word_index)\nword2idx_en = tokenizer_en.word_index\nidx2word_en = {v:k for k, v in word2idx_en.items()}\nword2idx_fr = tokenizer_fr.word_index\nidx2word_fr = {v:k for k, v in word2idx_fr.items()}\nprint(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\n    vocab_size_en, vocab_size_fr))\n\nmaxlen_en = data_en.shape[1]\nmaxlen_fr = data_fr_out.shape[1]\nprint(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))\n\nbatch_size = BATCH_SIZE\ndataset = tf.data.Dataset.from_tensor_slices((data_en, data_fr_in, data_fr_out))\ndataset = dataset.shuffle(10000)\ntest_size = NUM_SENT_PAIRS // 4\ntest_dataset = dataset.take(test_size).batch(batch_size, drop_remainder=True)\ntrain_dataset = dataset.skip(test_size).batch(batch_size, drop_remainder=True)\n\n# check encoder/decoder dimensions\nembedding_dim = EMBEDDING_DIM\nencoder_dim, decoder_dim = ENCODER_DIM, DECODER_DIM\n\nencoder = Encoder(vocab_size_en+1, embedding_dim, maxlen_en, encoder_dim)\ndecoder = Decoder(vocab_size_fr+1, embedding_dim, maxlen_fr, decoder_dim)\n\n# for encoder_in, decoder_in, decoder_out in train_dataset:\n#     encoder_state = encoder.init_state(batch_size)\n#     encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n#     decoder_state = encoder_state\n#     decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n#     break\n# print(\"encoder input          :\", encoder_in.shape)\n# print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\n# print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\n# print(\"decoder output (labels):\", decoder_out.shape)\n# # encoder input          : (64, 8)\n# # encoder output         : (64, 1024) state: (64, 1024)\n# # decoder output (logits): (64, 16, 7658) state: (64, 1024)\n# # decoder output (labels): (64, 16)\n\noptimizer = tf.keras.optimizers.Adam()\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)\n\nnum_epochs = NUM_EPOCHS\neval_scores = []\n\nfor e in range(num_epochs):\n    encoder_state = encoder.init_state(batch_size)\n\n    for batch, data in enumerate(train_dataset):\n        encoder_in, decoder_in, decoder_out = data\n        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n        loss = train_step(\n            encoder_in, decoder_in, decoder_out, encoder_state)\n    \n    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n\n    if e % 10 == 0:\n        checkpoint.save(file_prefix=checkpoint_prefix)\n    \n    predict(encoder, decoder, batch_size, sents_en, data_en,\n        sents_fr_out, word2idx_fr, idx2word_fr)\n\n    eval_score = evaluate_bleu_score(encoder, decoder, test_dataset, word2idx_fr, idx2word_fr)\n    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n    # eval_scores.append(eval_score)\n\ncheckpoint.save(file_prefix=checkpoint_prefix)\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}