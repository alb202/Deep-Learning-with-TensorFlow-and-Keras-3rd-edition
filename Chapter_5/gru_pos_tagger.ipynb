{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\n\n\ndef clean_logs(data_dir):\n    logs_dir = os.path.join(data_dir, \"logs\")\n    shutil.rmtree(logs_dir, ignore_errors=True)\n    return logs_dir\n\n\ndef download_and_read(dataset_dir, num_pairs=None):\n    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n        import nltk    \n\n        if not os.path.exists(dataset_dir):\n            os.makedirs(dataset_dir)\n        fsents = open(sent_filename, \"w\")\n        fposs = open(poss_filename, \"w\")\n        sentences = nltk.corpus.treebank.tagged_sents()\n        for sent in sentences:\n            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n\n        fsents.close()\n        fposs.close()\n    sents, poss = [], []\n    with open(sent_filename, \"r\") as fsent:\n        for idx, line in enumerate(fsent):\n            sents.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    with open(poss_filename, \"r\") as fposs:\n        for idx, line in enumerate(fposs):\n            poss.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    return sents, poss\n\n\ndef tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n    if vocab_size is None:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n    else:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n    tokenizer.fit_on_texts(texts)\n    if vocab_size is not None:\n        # additional workaround, see issue 8092\n        # https://github.com/keras-team/keras/issues/8092\n        tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items() \n            if i <= vocab_size+1 }\n    word2idx = tokenizer.word_index\n    idx2word = {v:k for k, v in word2idx.items()}\n    return word2idx, idx2word, tokenizer\n\n\nclass POSTaggingModel(tf.keras.Model):\n    def __init__(self, source_vocab_size, target_vocab_size,\n            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n        super(POSTaggingModel, self).__init__(**kwargs)\n        self.embed = tf.keras.layers.Embedding(\n            source_vocab_size, embedding_dim, input_length=max_seqlen)\n        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n        self.rnn = tf.keras.layers.Bidirectional(\n            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n        self.dense = tf.keras.layers.TimeDistributed(\n            tf.keras.layers.Dense(target_vocab_size))\n        self.activation = tf.keras.layers.Activation(\"softmax\")\n\n    def call(self, x):\n        x = self.embed(x)\n        x = self.dropout(x)\n        x = self.rnn(x)\n        x = self.dense(x)\n        x = self.activation(x)\n        return x\n\n\ndef masked_accuracy():\n    def masked_accuracy_fn(ytrue, ypred):\n        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n \n        mask = tf.keras.backend.cast(\n            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n        matches = tf.keras.backend.cast(\n            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n        numer = tf.keras.backend.sum(matches)\n        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n        accuracy =  numer / denom\n        return accuracy\n\n    return masked_accuracy_fn\n\n\nNUM_PAIRS = None\nEMBEDDING_DIM = 128\nRNN_OUTPUT_DIM = 256\nBATCH_SIZE = 128\nNUM_EPOCHS = 50\n\n# set random seed\ntf.random.set_seed(42)\n\n# clean up log area\ndata_dir = \"./data\"\nlogs_dir = clean_logs(data_dir)\n\n# download and read source and target data into data structure\nsents, poss = download_and_read(\"./datasets\", num_pairs=NUM_PAIRS)\nassert(len(sents) == len(poss))\nprint(\"# of records: {:d}\".format(len(sents)))\n\n# vocabulary sizes\nword2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\n    sents, vocab_size=9000)\nword2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\n    poss, vocab_size=38, lower=False)\nsource_vocab_size = len(word2idx_s)\ntarget_vocab_size = len(word2idx_t)\nprint(\"vocab sizes (source): {:d}, (target): {:d}\".format(\n    source_vocab_size, target_vocab_size))\n\n# # max sequence length - these should be identical on source and\n# # target so we can just analyze one of them and choose max_seqlen\n# sequence_lengths = np.array([len(s.split()) for s in sents])\n# print([(p, np.percentile(sequence_lengths, p)) \n#     for p in [75, 80, 90, 95, 99, 100]])\n# # [(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\nmax_seqlen = 271\n\n# create dataset\nsents_as_ints = tokenizer_s.texts_to_sequences(sents)\nsents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\nposs_as_ints = tokenizer_t.texts_to_sequences(poss)\nposs_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\ndataset = tf.data.Dataset.from_tensor_slices(\n    (sents_as_ints, poss_as_ints))\nidx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\nposs_as_catints = []\nfor p in poss_as_ints:\n    poss_as_catints.append(tf.keras.utils.to_categorical(p, \n        num_classes=target_vocab_size, dtype=\"int32\"))\nposs_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_catints, maxlen=max_seqlen)\ndataset = tf.data.Dataset.from_tensor_slices(\n    (sents_as_ints, poss_as_catints))\n\n# split into training, validation, and test datasets\ndataset = dataset.shuffle(10000)\ntest_size = len(sents) // 3\nval_size = (len(sents) - test_size) // 10\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\n\n# create batches\nbatch_size = BATCH_SIZE\ntrain_dataset = train_dataset.batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\ntest_dataset = test_dataset.batch(batch_size)\n\n# define model\nembedding_dim = EMBEDDING_DIM\nrnn_output_dim = RNN_OUTPUT_DIM\n\nmodel = POSTaggingModel(source_vocab_size, target_vocab_size,\n    embedding_dim, max_seqlen, rnn_output_dim)\nmodel.build(input_shape=(batch_size, max_seqlen))\nmodel.summary()\n\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=\"adam\", \n    metrics=[\"accuracy\", masked_accuracy()])\n\n# for input_b, output_b in train_dataset.take(1):\n#     pred_b = model(input_b)\n#     pred_b = tf.argmax(pred_b, axis=-1)\n# print(\"in:\", input_b.shape, \"label:\", output_b.shape, \n#     \"prediction:\", pred_b.shape)\n\n# train\nnum_epochs = NUM_EPOCHS\n\nbest_model_file = os.path.join(data_dir, \"best_model.h5\")\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    best_model_file, \n    save_weights_only=True,\n    save_best_only=True)\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\nhistory = model.fit(train_dataset, \n    epochs=num_epochs,\n    validation_data=val_dataset,\n    callbacks=[checkpoint, tensorboard])\n\n# evaluate with test set\nbest_model = POSTaggingModel(source_vocab_size, target_vocab_size,\n    embedding_dim, max_seqlen, rnn_output_dim)\nbest_model.build(input_shape=(batch_size, max_seqlen))\nbest_model.load_weights(best_model_file)\nbest_model.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=\"adam\", \n    metrics=[\"accuracy\", masked_accuracy()])\n\ntest_loss, test_acc, test_masked_acc = best_model.evaluate(test_dataset)\nprint(\"test loss: {:.3f}, test accuracy: {:.3f}, masked test accuracy: {:.3f}\".format(\n    test_loss, test_acc, test_masked_acc))\n\n# predict on batches\nlabels, predictions = [], []\nis_first_batch = True\naccuracies = []\n\nfor test_batch in test_dataset:\n    inputs_b, outputs_b = test_batch\n    preds_b = best_model.predict(inputs_b)\n    # convert from categorical to list of ints\n    preds_b = np.argmax(preds_b, axis=-1)\n    outputs_b = np.argmax(outputs_b.numpy(), axis=-1)\n    for i, (pred_l, output_l) in enumerate(zip(preds_b, outputs_b)):\n        assert(len(pred_l) == len(output_l))\n        pad_len = np.nonzero(output_l)[0][0]\n        acc = np.count_nonzero(\n            np.equal(\n                output_l[pad_len:], pred_l[pad_len:]\n            )\n        ) / len(output_l[pad_len:])\n        accuracies.append(acc)\n        if is_first_batch:\n            words = [idx2word_s[x] for x in inputs_b.numpy()[i][pad_len:]]\n            postags_l = [idx2word_t[x] for x in output_l[pad_len:] if x > 0]\n            postags_p = [idx2word_t[x] for x in pred_l[pad_len:] if x > 0]\n            print(\"labeled  : {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n                for (w, p) in zip(words, postags_l)])))\n            print(\"predicted: {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p) \n                for (w, p) in zip(words, postags_p)])))\n            print(\" \")\n    is_first_batch = False\n\naccuracy_score = np.mean(np.array(accuracies))\nprint(\"pos tagging accuracy: {:.3f}\".format(accuracy_score))\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}