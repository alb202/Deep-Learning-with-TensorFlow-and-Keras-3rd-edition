{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import os\nimport numpy as np\nimport re\nimport shutil\nimport tensorflow as tf\n\nDATA_DIR = \"./data\"\nCHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\nLOG_DIR = os.path.join(DATA_DIR, \"logs\")\n\n\ndef clean_logs():\n    shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n    shutil.rmtree(LOG_DIR, ignore_errors=True)\n\n\ndef download_and_read(urls):\n    texts = []\n    for i, url in enumerate(urls):\n        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url,\n            cache_dir=\".\")\n        text = open(p, mode=\"r\", encoding=\"utf-8\").read()\n        # remove byte order mark\n        text = text.replace(\"\\ufeff\", \"\")\n        # remove newlines\n        text = text.replace('\\n', ' ')\n        text = re.sub(r'\\s+', \" \", text)\n        # add it to the list\n        texts.extend(text)\n    return texts\n\n\ndef split_train_labels(sequence):\n    input_seq = sequence[0:-1]\n    output_seq = sequence[1:]\n    return input_seq, output_seq\n\n\nclass CharGenModel(tf.keras.Model):\n\n    def __init__(self, vocab_size, num_timesteps, \n            embedding_dim, **kwargs):\n        super(CharGenModel, self).__init__(**kwargs)\n        self.embedding_layer = tf.keras.layers.Embedding(\n            vocab_size,\n            embedding_dim\n        )\n        self.rnn_layer = tf.keras.layers.GRU(\n            num_timesteps,\n            recurrent_initializer=\"glorot_uniform\",\n            recurrent_activation=\"sigmoid\",\n            stateful=True,\n            return_sequences=True\n        )\n        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n\n    def call(self, x):\n        x = self.embedding_layer(x)\n        x = self.rnn_layer(x)\n        x = self.dense_layer(x)\n        return x\n\n\ndef loss(labels, predictions):\n    return tf.losses.sparse_categorical_crossentropy(\n        labels,\n        predictions,\n        from_logits=True\n    )\n\n\ndef generate_text(model, prefix_string, char2idx, idx2char,\n        num_chars_to_generate=1000, temperature=1.0):\n    input = [char2idx[s] for s in prefix_string]\n    input = tf.expand_dims(input, 0)\n    text_generated = []\n    model.reset_states()\n    for i in range(num_chars_to_generate):\n        preds = model(input)\n        preds = tf.squeeze(preds, 0) / temperature\n        # predict char returned by model\n        pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n        text_generated.append(idx2char[pred_id])\n        # pass the prediction as the next input to the model\n        input = tf.expand_dims([pred_id], 0)\n\n    return prefix_string + \"\".join(text_generated)\n\n\n# download and read into local data structure (list of chars)\ntexts = download_and_read([\n    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n    \"https://www.gutenberg.org/files/12/12-0.txt\"\n])\nclean_logs()\n\n# create the vocabulary\nvocab = sorted(set(texts))\nprint(\"vocab size: {:d}\".format(len(vocab)))\n\n# create mapping from vocab chars to ints\nchar2idx = {c:i for i, c in enumerate(vocab)}\nidx2char = {i:c for c, i in char2idx.items()}\n\n# numericize the texts\ntexts_as_ints = np.array([char2idx[c] for c in texts])\ndata = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n\n# number of characters to show before asking for prediction\n# sequences: [None, 100]\nseq_length = 100\nsequences = data.batch(seq_length + 1, drop_remainder=True)\nsequences = sequences.map(split_train_labels)\n\n# print out input and output to see what they look like\nfor input_seq, output_seq in sequences.take(1):\n    print(\"input:[{:s}]\".format(\n        \"\".join([idx2char[i] for i in input_seq.numpy()])))\n    print(\"output:[{:s}]\".format(\n        \"\".join([idx2char[i] for i in output_seq.numpy()])))\n\n# set up for training\n# batches: [None, 64, 100]\nbatch_size = 64\nsteps_per_epoch = len(texts) // seq_length // batch_size\ndataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)\nprint(dataset)\n\n# define network\nvocab_size = len(vocab)\nembedding_dim = 256\n\nmodel = CharGenModel(vocab_size, seq_length, embedding_dim)\nmodel.build(input_shape=(batch_size, seq_length))\nmodel.summary()\n\n# try running some data through the model to validate dimensions\nfor input_batch, label_batch in dataset.take(1):\n    pred_batch = model(input_batch)\n\nprint(pred_batch.shape)\nassert(pred_batch.shape[0] == batch_size)\nassert(pred_batch.shape[1] == seq_length)\nassert(pred_batch.shape[2] == vocab_size)\n\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=loss)\n\n# we will train our model for 50 epochs, and after every 10 epochs\n# we want to see how well it will generate text\nnum_epochs = 50\nfor i in range(num_epochs // 10):\n    model.fit(\n        dataset.repeat(),\n        epochs=10,\n        steps_per_epoch=steps_per_epoch\n        # callbacks=[checkpoint_callback, tensorboard_callback]\n    )\n    checkpoint_file = os.path.join(\n        CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n    model.save_weights(checkpoint_file)\n\n    # create a generative model using the trained model so far\n    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n    gen_model.load_weights(checkpoint_file)\n    gen_model.build(input_shape=(1, seq_length))\n\n    print(\"after epoch: {:d}\".format(i+1)*10)\n    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n    print(\"---\")\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}