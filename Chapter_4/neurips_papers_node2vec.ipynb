{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import gensim\nimport logging\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\n\nfrom scipy.sparse import csr_matrix\n# from scipy.stats import spearmanr\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nDATA_DIR = \"./data\"\nUCI_DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv\"\n\nNUM_WALKS_PER_VERTEX = 32\nMAX_PATH_LENGTH = 40\nRESTART_PROB = 0.15\n\nRANDOM_WALKS_FILE = os.path.join(DATA_DIR, \"random-walks.txt\")\nW2V_MODEL_FILE = os.path.join(DATA_DIR, \"w2v-neurips-papers.model\")\n\ndef download_and_read(url):\n    local_file = url.split('/')[-1]\n    p = tf.keras.utils.get_file(local_file, url, cache_dir=\".\")\n    row_ids, col_ids, data = [], [], []\n    rid = 0\n    f = open(p, \"r\")\n    for line in f:\n        line = line.strip()\n        if line.startswith(\"\\\"\\\",\"):\n            # header\n            continue\n        if rid % 100 == 0:\n            print(\"{:d} rows read\".format(rid))\n        # compute non-zero elements for current row\n        counts = np.array([int(x) for x in line.split(',')[1:]])\n        nz_col_ids = np.nonzero(counts)[0]\n        nz_data = counts[nz_col_ids]\n        nz_row_ids = np.repeat(rid, len(nz_col_ids))\n        rid += 1\n        # add data to big lists\n        row_ids.extend(nz_row_ids.tolist())\n        col_ids.extend(nz_col_ids.tolist())\n        data.extend(nz_data.tolist())\n    print(\"{:d} rows read, COMPLETE\".format(rid))\n    f.close()\n    TD = csr_matrix((\n        np.array(data), (\n            np.array(row_ids), np.array(col_ids)\n            )\n        ),\n        shape=(rid, counts.shape[0]))\n    return TD\n\n\ndef construct_random_walks(E, n, alpha, l, ofile):\n    \"\"\" NOTE: takes a long time to do, consider using some parallelization\n        for larger problems.\n    \"\"\"\n    if os.path.exists(ofile):\n        print(\"random walks generated already, skipping\")\n        return\n    f = open(ofile, \"w\")\n    for i in range(E.shape[0]):  # for each vertex\n        if i % 100 == 0:\n            print(\"{:d} random walks generated from {:d} starting vertices\"\n                .format(n * i, i))\n        if i <= 3273:\n            continue\n        for j in range(n):       # construct n random walks\n            curr = i\n            walk = [curr]\n            target_nodes = np.nonzero(E[curr])[1]\n            for k in range(l):   # each of max length l, restart prob alpha\n                # should we restart?\n                if np.random.random() < alpha and len(walk) > 5:\n                    break\n                # choose one outgoing edge and append to walk\n                try:\n                    curr = np.random.choice(target_nodes)\n                    walk.append(curr)\n                    target_nodes = np.nonzero(E[curr])[1]\n                except ValueError:\n                    continue\n            f.write(\"{:s}\\n\".format(\" \".join([str(x) for x in walk])))\n\n    print(\"{:d} random walks generated from {:d} starting vertices, COMPLETE\"\n        .format(n * i, i))\n    f.close()\n\n\nclass Documents(object):\n    def __init__(self, input_file):\n        self.input_file = input_file\n\n    def __iter__(self):\n        with open(self.input_file, \"r\") as f:\n            for i, line in enumerate(f):\n                if i % 1000 == 0:\n                    if i % 1000 == 0:\n                        logging.info(\"{:d} random walks extracted\".format(i))\n                yield line.strip().split()\n\n\ndef train_word2vec_model(random_walks_file, model_file):\n    if os.path.exists(model_file):\n        print(\"Model file {:s} already present, skipping training\"\n            .format(model_file))\n        return\n    docs = Documents(random_walks_file)\n    model = gensim.models.Word2Vec(\n        docs,\n        size=128,    # size of embedding vector\n        window=10,   # window size\n        sg=1,        # skip-gram model\n        min_count=2,\n        workers=4\n    )\n    model.train(\n        docs, \n        total_examples=model.corpus_count,\n        epochs=50)\n    model.save(model_file)\n\n\n# def evaluate_model_file(td_matrix, model_file, source_node_ids):\n#     model = gensim.models.Word2Vec.load(model_file).wv\n#     for source_node_id in source_node_ids:\n#         most_similar = model.most_similar(str(source_node_id))\n#         scores = [x[1] for x in most_similar]\n#         target_ids = [x[0] for x in most_similar]\n#         X = np.repeat(td_matrix[source_node_id].todense(), 10, axis=0)\n#         Y = td_matrix[target_ids].todense()\n#         cosims = [cosine_similarity(X[i], Y[i])[0, 0] for i in range(X.shape[0])]\n#         rank_corr = spearmanr(scores, cosims, axis=0)[0]\n#         print(\"{:d}\\t{:.5f}\".format(source_node_id, rank_corr))\n\ndef evaluate_model(td_matrix, model_file, source_id):\n    model = gensim.models.Word2Vec.load(model_file).wv\n    most_similar = model.most_similar(str(source_id))\n    scores = [x[1] for x in most_similar]\n    target_ids = [x[0] for x in most_similar]\n    # compare top 10 scores with cosine similarity between source and each target\n    X = np.repeat(td_matrix[source_id].todense(), 10, axis=0)\n    Y = td_matrix[target_ids].todense()\n    cosims = [cosine_similarity(X[i], Y[i])[0, 0] for i in range(10)]\n    for i in range(10):\n        print(\"{:d} {:s} {:.3f} {:.3f}\".format(\n            source_id, target_ids[i], cosims[i], scores[i]))\n\n\n# read data and convert to Term-Document matrix\nTD = download_and_read(UCI_DATA_URL)\n# compute undirected, unweighted edge matrix\nE = TD.T * TD\n# binarize\nE[E > 0] = 1\nprint(E.shape)\n\n# construct random walks (caution: long process!)\nconstruct_random_walks(E, NUM_WALKS_PER_VERTEX, RESTART_PROB, \n    MAX_PATH_LENGTH, RANDOM_WALKS_FILE)\n\n# train model\ntrain_word2vec_model(RANDOM_WALKS_FILE, W2V_MODEL_FILE)\n\n# evaluate\nsource_id = np.random.choice(E.shape[0])\nevaluate_model(TD, W2V_MODEL_FILE, source_id)\n\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}