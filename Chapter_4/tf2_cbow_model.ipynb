{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import tensorflow as tf\n\nclass CBOWModel(tf.keras.Model):\n    def __init__(self, vocab_sz, emb_sz, window_sz, **kwargs):\n        super(CBOWModel, self).__init__(**kwargs)\n        self.embedding = tf.keras.layers.Embedding(\n            input_dim=vocab_sz,\n            output_dim=emb_sz,\n            embeddings_initializer=\"glorot_uniform\",\n            input_length=window_sz*2\n        )\n        self.dense = tf.keras.layers.Dense(\n            vocab_sz,\n            kernel_initializer=\"glorot_uniform\",\n            activation=\"softmax\"\n        )\n\n    def call(self, x):\n        x = self.embedding(x)\n        x = tf.reduce_mean(x, axis=1)\n        x = self.dense(x)\n        return x\n\n\nVOCAB_SIZE = 5000\nEMBED_SIZE = 300\nWINDOW_SIZE = 1  # 3 word window, 1 on left, 1 on right\n\nmodel = CBOWModel(VOCAB_SIZE, EMBED_SIZE, WINDOW_SIZE)\nmodel.build(input_shape=(None, VOCAB_SIZE))\nmodel.compile(optimizer=tf.optimizers.Adam(),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"])\n\nmodel.summary()\n\n# train the model here\n\n# retrieve embeddings from trained model\nemb_layer = [layer for layer in model.layers \n    if layer.name.startswith(\"embedding\")][0]\nemb_weight = [weight.numpy() for weight in emb_layer.weights\n    if weight.name.endswith(\"/embeddings:0\")][0]\nprint(emb_weight, emb_weight.shape)\n\n\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}