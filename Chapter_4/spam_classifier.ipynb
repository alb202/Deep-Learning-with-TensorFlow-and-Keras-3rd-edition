{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import argparse\nimport gensim.downloader as api\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef download_and_read(url):\n    local_file = url.split('/')[-1]\n    p = tf.keras.utils.get_file(local_file, url, \n        extract=True, cache_dir=\".\")\n    labels, texts = [], []\n    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n    with open(local_file, \"r\") as fin:\n        for line in fin:\n            label, text = line.strip().split('\\t')\n            labels.append(1 if label == \"spam\" else 0)\n            texts.append(text)\n    return texts, labels\n\n\ndef build_embedding_matrix(sequences, word2idx, embedding_dim, \n        embedding_file):\n    if os.path.exists(embedding_file):\n        E = np.load(embedding_file)\n    else:\n        vocab_size = len(word2idx)\n        E = np.zeros((vocab_size, embedding_dim))\n        word_vectors = api.load(EMBEDDING_MODEL)\n        for word, idx in word2idx.items():\n            try:\n                E[idx] = word_vectors.word_vec(word)\n            except KeyError:   # word not in embedding\n                pass\n            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n            #     pass\n        np.save(embedding_file, E)\n    return E\n\n\nclass SpamClassifierModel(tf.keras.Model):\n    def __init__(self, vocab_sz, embed_sz, input_length,\n            num_filters, kernel_sz, output_sz, \n            run_mode, embedding_weights, \n            **kwargs):\n        super(SpamClassifierModel, self).__init__(**kwargs)\n        if run_mode == \"scratch\":\n            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n                embed_sz,\n                input_length=input_length,\n                trainable=True)\n        elif run_mode == \"vectorizer\":\n            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n                embed_sz,\n                input_length=input_length,\n                weights=[embedding_weights],\n                trainable=False)\n        else:\n            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n                embed_sz,\n                input_length=input_length,\n                weights=[embedding_weights],\n                trainable=True)\n        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n        self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n            kernel_size=kernel_sz,\n            activation=\"relu\")\n        self.pool = tf.keras.layers.GlobalMaxPooling1D()\n        self.dense = tf.keras.layers.Dense(output_sz, \n            activation=\"softmax\"\n        )\n\n    def call(self, x):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        x = self.conv(x)\n        x = self.pool(x)\n        x = self.dense(x)\n        return x\n\n\nDATA_DIR = \"data\"\nEMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\nDATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\nEMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\nEMBEDDING_DIM = 300\nNUM_CLASSES = 2\nBATCH_SIZE = 128\nNUM_EPOCHS = 3\n\n# data distribution is 4827 ham and 747 spam (total 5574), which \n# works out to approx 87% ham and 13% spam, so we take reciprocals\n# and this works out to being each spam (1) item as being approximately\n# 8 times as important as each ham (0) message.\nCLASS_WEIGHTS = { 0: 1, 1: 8 }\n\ntf.random.set_seed(42)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--mode\", help=\"run mode\",\n    choices=[\n        \"scratch\",\n        \"vectorizer\",\n        \"finetuning\"\n    ])\nargs = parser.parse_args()\nrun_mode = args.mode\n\n# read data\ntexts, labels = download_and_read(DATASET_URL)\n\n# tokenize and pad text\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(texts)\ntext_sequences = tokenizer.texts_to_sequences(texts)\ntext_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\nnum_records = len(text_sequences)\nmax_seqlen = len(text_sequences[0])\nprint(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))\n\n# labels\ncat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n\n# vocabulary\nword2idx = tokenizer.word_index\nidx2word = {v:k for k, v in word2idx.items()}\nword2idx[\"PAD\"] = 0\nidx2word[0] = \"PAD\"\nvocab_size = len(word2idx)\nprint(\"vocab size: {:d}\".format(vocab_size))\n\n# dataset\ndataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\ndataset = dataset.shuffle(10000)\ntest_size = num_records // 4\nval_size = (num_records - test_size) // 10\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\n\ntest_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\nval_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n\n# embedding\nE = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n    EMBEDDING_NUMPY_FILE)\nprint(\"Embedding matrix:\", E.shape)\n\n# model definition\nconv_num_filters = 256\nconv_kernel_size = 3\nmodel = SpamClassifierModel(\n    vocab_size, EMBEDDING_DIM, max_seqlen, \n    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n    run_mode, E)\nmodel.build(input_shape=(None, max_seqlen))\nmodel.summary()\n\n# compile and train\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"])\n\n# train model\nmodel.fit(train_dataset, epochs=NUM_EPOCHS, \n    validation_data=val_dataset,\n    class_weight=CLASS_WEIGHTS)\n\n# evaluate against test set\nlabels, predictions = [], []\nfor Xtest, Ytest in test_dataset:\n    Ytest_ = model.predict_on_batch(Xtest)\n    ytest = np.argmax(Ytest, axis=1)\n    ytest_ = np.argmax(Ytest_, axis=1)\n    labels.extend(ytest.tolist())\n    predictions.extend(ytest.tolist())\n\nprint(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\nprint(\"confusion matrix\")\nprint(confusion_matrix(labels, predictions))\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}