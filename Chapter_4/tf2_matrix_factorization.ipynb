{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["import numpy as np\nimport tensorflow as tf\n\nclass MatrixFactorizationLayer(tf.keras.layers.Layer):\n    def __init__(self, emb_sz, **kwargs):\n        super(MatrixFactorizationLayer, self).__init__(**kwargs)\n        self.emb_sz = emb_sz\n\n    def build(self, input_shape):\n        num_rows, num_cols = input_shape\n        self.P = self.add_variable(\"P\", \n            shape=[num_rows, self.emb_sz], \n            dtype=tf.float32,\n            initializer=tf.initializers.GlorotUniform)\n        self.Q = self.add_variable(\"Q\", \n            shape=[num_cols, self.emb_sz],\n            dtype=tf.float32, \n            initializer=tf.initializers.GlorotUniform)\n\n    def call(self, input):\n        return tf.matmul(self.P, tf.transpose(self.Q))\n\n\nclass MatrixFactorizationModel(tf.keras.Model):\n    def __init__(self, embedding_size):\n        super(MatrixFactorizationModel, self).__init__()\n        self.mfl = MatrixFactorizationLayer(embedding_size)\n        self.sigmoid = tf.keras.layers.Activation(\"sigmoid\")\n\n    def call(self, x):\n        x = self.mfl(x)\n        x = self.sigmoid(x)\n        return x\n\n\ndef loss_fn(source, target):\n    mse = tf.keras.losses.MeanSquaredError()\n    loss = mse(source, target)\n    return loss\n\n\nEMBEDDING_SIZE = 15\nNUM_ROWS = 1000\nNUM_COLS = 5000\n\n# this is the input matrix R, which we are currently spoofing\n# with a random matrix (this should be sparse)\nR = np.random.random((NUM_ROWS, NUM_COLS))\n\nmodel = MatrixFactorizationModel(EMBEDDING_SIZE)\nmodel.build(input_shape=R.shape)\nmodel.summary()\n\noptimizer = tf.optimizers.RMSprop(learning_rate=1e-3, momentum=0.9)\n\n# # train model\n# losses, steps = [], []\n# for i in range(5000):\n#     with tf.GradientTape() as tape:\n#         Rprime = model(R)\n#         loss = loss_fn(R, Rprime)\n#         if i % 100 == 0:\n#             loss_value = loss.numpy()\n#             losses.append(loss_value)\n#             steps.append(i)\n#             print(\"step: {:d}, loss: {:.3f}\".format(i, loss_value))\n#     variables = model.trainable_variables\n#     gradients = tape.gradient(loss, variables)\n#     optimizer.apply_gradients(zip(gradients, variables))\n\n# after training, retrieve P and Q\nmf_layer = model.layers[0]\nP, Q = [weight.numpy() for weight in mf_layer.weights]\nprint(P.shape, Q.shape)\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}