{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["# -*- coding: utf-8 -*-\n\"\"\"packt-18-dgl-graph-classification.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1v4RxLs4L4nCH30CLM4SP_s1TX9Loiikc\n\n# Graph Classification with DGL\n\nTensorflow and DGL based re-implementation of the PyTorch and DGL based example at [Training a GNN for Graph Classification](https://docs.dgl.ai/tutorials/blitz/5_graph_classification.html#sphx-glr-tutorials-blitz-5-graph-classification-py)\n\"\"\"\n\n# Commented out IPython magic to ensure Python compatibility.\n# %env DGLBACKEND=tensorflow\n\n!pip install dgl tensorflow_addons\n\nimport dgl.data\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom dgl.nn import GraphConv\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"## Loading Data\n\nLabel: binary, whether protein is an enzyme or not.\n\"\"\"\n\ndataset = dgl.data.GINDataset(\"PROTEINS\", self_loop=True)\n\nprint(\"node feature dimensionality:\", dataset.dim_nfeats)\nprint(\"number of graph categories:\", dataset.gclasses)\nprint(\"number of graphs in dataset:\", len(dataset))\n\n\"\"\"## Split Dataset into Train and Test\"\"\"\n\ntv_dataset, test_dataset = train_test_split(dataset, shuffle=True, test_size=0.2)\ntrain_dataset, val_dataset = train_test_split(tv_dataset, test_size=0.1)\nprint(len(train_dataset), len(val_dataset), len(test_dataset))\n\n\"\"\"## Define Model\"\"\"\n\nclass GraphClassifier(tf.keras.Model):\n  def __init__(self, in_feats, h_feats, num_classes):\n    super(GraphClassifier, self).__init__()\n    self.conv1 = GraphConv(in_feats, h_feats, activation=tf.nn.relu)\n    self.conv2 = GraphConv(h_feats, num_classes)\n\n  def call(self, g, in_feat):\n    h = self.conv1(g, in_feat)\n    h = self.conv2(g, h)\n    g.ndata[\"h\"] = h\n    return dgl.mean_nodes(g, \"h\")\n\n\nmodel = GraphClassifier(dataset.dim_nfeats, 16, dataset.gclasses)\ngraphs, labels = zip(*[dataset[i] for i in range(16)])\nbatched_graphs = dgl.batch(graphs)\nbatched_labels = tf.convert_to_tensor(labels)\npred = model(batched_graphs, batched_graphs.ndata[\"attr\"])\nprint(pred.shape)\n\n\"\"\"## Training Loop\"\"\"\n\nHIDDEN_SIZE = 16\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-2\nNUM_EPOCHS = 20\n\ndef set_gpu_if_available():\n  device = \"/cpu:0\"\n  gpus = tf.config.list_physical_devices(\"GPU\")\n  if len(gpus) > 0:\n    device = gpus[0]\n  return device\n\ndevice = set_gpu_if_available()\n\ndef do_eval(model, dataset):\n  total_acc, total_recs = 0, 0\n  indexes = tf.data.Dataset.from_tensor_slices(range(len(dataset)))\n  indexes = indexes.batch(batch_size=BATCH_SIZE)\n\n  for batched_indexes in indexes:\n    graphs, labels = zip(*[dataset[i] for i in batched_indexes])\n    batched_graphs = dgl.batch(graphs)\n    batched_labels = tf.convert_to_tensor(labels, dtype=tf.int64)\n    batched_graphs = batched_graphs.to(device)\n    logits = model(batched_graphs, batched_graphs.ndata[\"attr\"])\n    batched_preds = tf.math.argmax(logits, axis=1)\n    acc = tf.reduce_sum(tf.cast(batched_preds == batched_labels, \n                                 dtype=tf.float32))\n    total_acc += acc.numpy().item()\n    total_recs += len(batched_labels)\n  \n  return total_acc / total_recs\n\nwith tf.device(device):\n  model = GraphClassifier(dataset.dim_nfeats, HIDDEN_SIZE, dataset.gclasses)\n  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n  loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n  train_indexes = tf.data.Dataset.from_tensor_slices(range(len(train_dataset)))\n  train_indexes = train_indexes.batch(batch_size=BATCH_SIZE)\n\n  for epoch in range(NUM_EPOCHS):\n    total_loss = 0\n    for batched_indexes in train_indexes:\n      with tf.GradientTape() as tape:\n        graphs, labels = zip(*[train_dataset[i] for i in batched_indexes])\n        batched_graphs = dgl.batch(graphs)\n        batched_labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n        batched_graphs = batched_graphs.to(device)\n        logits = model(batched_graphs, batched_graphs.ndata[\"attr\"])\n        loss = loss_fcn(batched_labels, logits)\n        grads = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n        total_loss += loss.numpy().item()\n    val_acc = do_eval(model, val_dataset)\n    print(\"Epoch {:3d} | train_loss: {:.3f} | val_acc: {:.3f}\".format(\n        epoch, total_loss, val_acc))\n\ntest_acc = do_eval(model, test_dataset)\nprint(\"test accuracy: {:.3f}\".format(test_acc))\n\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}