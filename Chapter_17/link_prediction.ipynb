{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["# -*- coding: utf-8 -*-\n\"\"\"packt-18-link-prediction.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1enEWv4rEpzLze8Hj4C2acls4r-En9SSM\n\n# Link Prediction\n\nTensorflow and DGL re-implementation of PyTorch and DGL example [Link Prediction using Graph Neural Networks](https://docs.dgl.ai/tutorials/blitz/4_link_predict.html#sphx-glr-tutorials-blitz-4-link-predict-py).\n\"\"\"\n\n# Commented out IPython magic to ensure Python compatibility.\n# %env DGLBACKEND=tensorflow\n\n# !pip install dgl\n\nimport dgl\nimport dgl.data\nimport dgl.function as fn\nimport tensorflow as tf\nimport itertools\nimport numpy as np\nimport scipy.sparse as sp\n\nfrom dgl.nn import SAGEConv\nfrom sklearn.metrics import roc_auc_score\n\n\"\"\"## Loading Graph and Features\"\"\"\n\ndataset = dgl.data.CoraGraphDataset()\ng = dataset[0]\ng\n\n\"\"\"## Prepare training and test sets\"\"\"\n\n# Split edge set for training and testing\nu, v = g.edges()\n\n# positive edges\neids = np.arange(g.number_of_edges())\neids = np.random.permutation(eids)\n\ntest_size = int(len(eids) * 0.2)\nval_size = int((len(eids) - test_size) * 0.1)\ntrain_size = g.number_of_edges() - test_size - val_size\n\nu = u.numpy()\nv = v.numpy()\n\ntest_pos_u = u[eids[0:test_size]]\ntest_pos_v = v[eids[0:test_size]]\nval_pos_u = u[eids[test_size:test_size + val_size]]\nval_pos_v = v[eids[test_size:test_size + val_size]]\ntrain_pos_u = u[eids[test_size + val_size:]]\ntrain_pos_v = v[eids[test_size + val_size:]]\n\nprint(train_pos_u.shape, train_pos_v.shape, \n      val_pos_u.shape, val_pos_v.shape,\n      test_pos_u.shape, test_pos_v.shape)\n\n# negative edges\nadj = sp.coo_matrix((np.ones(len(u)), (u, v)))\nadj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\nneg_u, neg_v = np.where(adj_neg != 0)\n\nneg_eids = np.random.choice(len(neg_u), g.number_of_edges())\ntest_neg_u = neg_u[neg_eids[:test_size]]\ntest_neg_v = neg_v[neg_eids[:test_size]]\nval_neg_u = neg_u[neg_eids[test_size:test_size + val_size]]\nval_neg_v = neg_v[neg_eids[test_size:test_size + val_size]]\ntrain_neg_u = neg_u[neg_eids[test_size + val_size:]]\ntrain_neg_v = neg_v[neg_eids[test_size + val_size:]]\n\nprint(train_neg_u.shape, train_neg_v.shape, \n      val_neg_u.shape, val_neg_v.shape,\n      test_neg_u.shape, test_neg_v.shape)\n\n# remove edges from training graph\ntest_edges = eids[:test_size]\nval_edges = eids[test_size:test_size + val_size]\ntrain_edges = eids[test_size + val_size:]\ntrain_g = dgl.remove_edges(g, np.concatenate([test_edges, val_edges]))\n\n\"\"\"## Define a GraphSAGE Model\"\"\"\n\nclass LinkPredictor(tf.keras.Model):\n  def __init__(self, g, in_feats, h_feats):\n    super(LinkPredictor, self).__init__()\n    self.g = g\n    self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n    self.relu1 = tf.keras.layers.Activation(tf.nn.relu)\n    self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n\n  def call(self, in_feat):\n    h = self.conv1(self.g, in_feat)\n    h = self.relu1(h)\n    h = self.conv2(self.g, h)\n    return h\n\ntrain_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())\ntrain_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n\nval_pos_g = dgl.graph((val_pos_u, val_pos_v), num_nodes=g.number_of_nodes())\nval_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())\n\ntest_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())\ntest_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n\nclass DotProductPredictor(tf.keras.Model):\n  def call(self, g, h):\n    with g.local_scope():\n      g.ndata['h'] = h\n      # Compute a new edge feature named 'score' by a dot-product between the\n      # source node feature 'h' and destination node feature 'h'.\n      g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n      # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n      return g.edata['score'][:, 0]\n\nclass MLPPredictor(tf.keras.Model):\n  def __init__(self, h_feats):\n    super().__init__()\n    self.W1 = tf.keras.layers.Dense(h_feats, activation=tf.nn.relu)\n    self.W2 = tf.keras.layers.Dense(1)\n\n  def apply_edges(self, edges):\n    \"\"\"\n    Computes a scalar score for each edge of the given graph.\n\n    Parameters\n    ----------\n    edges :\n      Has three members ``src``, ``dst`` and ``data``, each of\n      which is a dictionary representing the features of the\n      source nodes, the destination nodes, and the edges\n      themselves.\n\n    Returns\n    -------\n    dict\n      A dictionary of new edge features.\n    \"\"\"\n    h = tf.concat([edges.src[\"h\"], edges.dst[\"h\"]], axis=1)\n    return {\n      \"score\": self.W2(self.W1(h))[:, 0]\n    }\n\n  def call(self, g, h):\n    with g.local_scope():\n      g.ndata['h'] = h\n      g.apply_edges(self.apply_edges)\n      return g.edata['score']\n\n\"\"\"## Training Loop (DotPredictor)\"\"\"\n\nHIDDEN_SIZE = 16\nLEARNING_RATE = 1e-2\nNUM_EPOCHS = 100\n\n# ----------- 3. set up loss and optimizer -------------- #\nmodel = LinkPredictor(train_g, train_g.ndata['feat'].shape[1], HIDDEN_SIZE)\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss_fcn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\npred = DotProductPredictor()\n\ndef compute_loss(pos_score, neg_score):\n    scores = tf.concat([pos_score, neg_score], axis=0)\n    labels = tf.concat([\n      tf.ones(pos_score.shape[0]), \n      tf.zeros(neg_score.shape[0])\n    ], axis=0)\n    return loss_fcn(labels, scores)\n\n\ndef compute_auc(pos_score, neg_score):\n    scores = tf.concat([pos_score, neg_score], axis=0).numpy()\n    labels = tf.concat([\n      tf.ones(pos_score.shape[0]), \n      tf.zeros(neg_score.shape[0])\n    ], axis=0).numpy()\n    return roc_auc_score(labels, scores)\n\n# ----------- 4. training -------------------------------- #\nfor epoch in range(NUM_EPOCHS):\n  in_feat = train_g.ndata[\"feat\"]\n  with tf.GradientTape() as tape:\n    h = model(in_feat)\n    pos_score = pred(train_pos_g, h)\n    neg_score = pred(train_neg_g, h)\n    loss = compute_loss(pos_score, neg_score)\n    grads = tape.gradient(loss, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n  val_pos_score = pred(val_pos_g, h)\n  val_neg_score = pred(val_neg_g, h)\n  val_auc = compute_auc(val_pos_score, val_neg_score)\n\n  if epoch % 5 == 0:\n    print(\"Epoch {:3d} | train_loss: {:.3f}, val_auc: {:.3f}\".format(\n        epoch, loss, val_auc))\n\n# ----------- 5. check results ------------------------ #\npos_score = tf.stop_gradient(pred(test_pos_g, h))\nneg_score = tf.stop_gradient(pred(test_neg_g, h))\nprint('Test AUC', compute_auc(pos_score, neg_score))\n\n\"\"\"## Training Loop (MLP Predictor)\"\"\"\n\n# ----------- 3. set up loss and optimizer -------------- #\nmodel = LinkPredictor(train_g, train_g.ndata['feat'].shape[1], HIDDEN_SIZE)\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss_fcn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\npred = MLPPredictor(HIDDEN_SIZE)\n\n# ----------- 4. training -------------------------------- #\nfor epoch in range(NUM_EPOCHS):\n  in_feat = train_g.ndata[\"feat\"]\n  with tf.GradientTape() as tape:\n    h = model(in_feat)\n    pos_score = pred(train_pos_g, h)\n    neg_score = pred(train_neg_g, h)\n    loss = compute_loss(pos_score, neg_score)\n    grads = tape.gradient(loss, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n  val_pos_score = pred(val_pos_g, h)\n  val_neg_score = pred(val_neg_g, h)\n  val_auc = compute_auc(val_pos_score, val_neg_score)\n\n  if epoch % 5 == 0:\n    print(\"Epoch {:3d} | train_loss: {:.3f}, val_auc: {:.3f}\".format(\n        epoch, loss, val_auc))\n\n# ----------- 5. check results ------------------------ #\npos_score = tf.stop_gradient(pred(test_pos_g, h))\nneg_score = tf.stop_gradient(pred(test_neg_g, h))\nprint('Test AUC', compute_auc(pos_score, neg_score))\n\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}