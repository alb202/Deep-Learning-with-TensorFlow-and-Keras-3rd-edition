{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": ["# -*- coding: utf-8 -*-\n\"\"\"packt-18-custom-layer.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1bD35UW02Quufx4n-eBN1RZzrM_Fyfx1_\n\n# Custom GNN Layer\n\nTensorflow and DGL re-implementation of PyTorch and DGL example at [Write your own GNN module](https://docs.dgl.ai/tutorials/blitz/3_message_passing.html#sphx-glr-tutorials-blitz-3-message-passing-py).\n\"\"\"\n\n# Commented out IPython magic to ensure Python compatibility.\n# %env DGLBACKEND=tensorflow\n\n!pip install dgl\n\nimport dgl\nimport dgl.data\nimport dgl.function as fn\nimport tensorflow as tf\n\n\"\"\"## Message passing and GNNs\"\"\"\n\nclass CustomGraphSAGE(tf.keras.layers.Layer):\n  \"\"\"Graph convolution module used by the GraphSAGE model.\n\n  Parameters\n  ----------\n  in_feat : int\n      Input feature size.\n  out_feat : int\n      Output feature size.\n  \"\"\"\n  def __init__(self, in_feat, out_feat):\n    super(CustomGraphSAGE, self).__init__()\n    # A linear submodule for projecting the input and neighbor feature to the output.\n    self.linear = tf.keras.layers.Dense(out_feat, activation=tf.nn.relu)\n\n  def call(self, g, h):\n      \"\"\"Forward computation\n\n      Parameters\n      ----------\n      g : Graph\n          The input graph.\n      h : Tensor\n          The input node feature.\n      \"\"\"\n      with g.local_scope():\n        g.ndata[\"h\"] = h\n        # update_all is a message passing API.\n        g.update_all(message_func=fn.copy_u('h', 'm'),\n                     reduce_func=fn.mean('m', 'h_N'))\n        h_N = g.ndata['h_N']\n        h_total = tf.concat([h, h_N], axis=1)\n        return self.linear(h_total)\n\nclass CustomGNN(tf.keras.Model):\n  def __init__(self, g, in_feats, h_feats, num_classes):\n    super(CustomGNN, self).__init__()\n    self.g = g\n    self.conv1 = CustomGraphSAGE(in_feats, h_feats)\n    self.relu1 = tf.keras.layers.Activation(tf.nn.relu)\n    self.conv2 = CustomGraphSAGE(h_feats, num_classes)\n\n  def call(self, in_feat):\n    h = self.conv1(self.g, in_feat)\n    h = self.relu1(h)\n    h = self.conv2(self.g, h)\n    return h\n\n\"\"\"## Training Loop\"\"\"\n\ndataset = dgl.data.CoraGraphDataset()\ng = dataset[0]\ng\n\nLEARNING_RATE = 1e-2\nNUM_EPOCHS = 200\n\ndef evaluate(model, features, labels, mask, edge_weights=None):\n  if edge_weights is None:\n    logits = model(features, training=False)\n  else:\n    logits = model(features, edge_weights, training=False)\n  logits = logits[mask]\n  labels = labels[mask]\n  indices = tf.math.argmax(logits, axis=1)\n  acc = tf.reduce_mean(tf.cast(indices == labels, dtype=tf.float32))\n  return acc.numpy().item()\n\n\ndef train(g, model, optimizer, loss_fcn, num_epochs, use_edge_weights=False):\n  features = g.ndata[\"feat\"]\n  labels = g.ndata[\"label\"]\n  if use_edge_weights:\n    edge_weights = g.edata[\"w\"]\n\n  train_mask = g.ndata[\"train_mask\"]\n  val_mask = g.ndata[\"val_mask\"]\n  test_mask = g.ndata[\"test_mask\"]\n  for epoch in range(num_epochs):\n    with tf.GradientTape() as tape:\n      if not use_edge_weights:\n        logits = model(features)\n      else:\n        logits = model(features, edge_weights)\n\n      loss_value = loss_fcn(labels[train_mask], logits[train_mask])\n      grads = tape.gradient(loss_value, model.trainable_weights)\n      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n    if not use_edge_weights:\n      acc = evaluate(model, features, labels, val_mask)\n    else:\n      acc = evaluate(model, features, labels, val_mask, edge_weights=edge_weights)\n    if epoch % 10 == 0:\n      print(\"Epoch {:5d} | loss: {:.3f} | val_acc: {:.3f}\".format(\n          epoch, loss_value.numpy().item(), acc))\n\n  if not use_edge_weights:\n    acc = evaluate(model, features, labels, test_mask)\n  else:\n    acc = evaluate(model, features, labels, test_mask, edge_weights=edge_weights)\n  print(\"Test accuracy: {:.3f}\".format(acc))\n\nmodel = CustomGNN(g, g.ndata['feat'].shape[1], 16, dataset.num_classes)\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\ntrain(g, model, optimizer, loss_fcn, NUM_EPOCHS)\n\n\"\"\"## More customization\"\"\"\n\nclass CustomWeightedGraphSAGE(tf.keras.layers.Layer):\n  \"\"\"Graph convolution module used by the GraphSAGE model with edge weights.\n\n  Parameters\n  ----------\n  in_feat : int\n      Input feature size.\n  out_feat : int\n      Output feature size.\n  \"\"\"\n  def __init__(self, in_feat, out_feat):\n    super(CustomWeightedGraphSAGE, self).__init__()\n    # A linear submodule for projecting the input and neighbor feature to the output.\n    self.linear = tf.keras.layers.Dense(out_feat, activation=tf.nn.relu)\n\n  def call(self, g, h, w):\n    \"\"\"Forward computation\n\n    Parameters\n    ----------\n    g : Graph\n        The input graph.\n    h : Tensor\n        The input node feature.\n    w : Tensor\n        The edge weight.\n    \"\"\"\n    with g.local_scope():\n      g.ndata['h'] = h\n      g.edata['w'] = w\n      g.update_all(message_func=fn.u_mul_e('h', 'w', 'm'),\n                   reduce_func=fn.mean('m', 'h_N'))\n      h_N = g.ndata['h_N']\n      h_total = tf.concat([h, h_N], axis=1)\n      return self.linear(h_total)\n\nclass CustomWeightedGNN(tf.keras.Model):\n  def __init__(self, g, in_feats, h_feats, num_classes):\n    super(CustomWeightedGNN, self).__init__()\n    self.g = g\n    self.conv1 = CustomWeightedGraphSAGE(in_feats, h_feats)\n    self.relu1 = tf.keras.layers.Activation(tf.nn.relu)\n    self.conv2 = CustomWeightedGraphSAGE(h_feats, num_classes)\n\n  def call(self, in_feat, edge_weights):\n    h = self.conv1(self.g, in_feat, edge_weights)\n    h = self.relu1(h)\n    h = self.conv2(self.g, h, edge_weights)\n    return h\n\ng.edata[\"w\"] = tf.cast(\n    tf.random.uniform((g.num_edges(), 1), minval=3, maxval=10, dtype=tf.int32),\n    dtype=tf.float32)\ng.edata[\"w\"]\n\nmodel = CustomWeightedGNN(g, g.ndata['feat'].shape[1], 16, dataset.num_classes)\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\ntrain(g, model, optimizer, loss_fcn, NUM_EPOCHS, use_edge_weights=True)\n\n\"\"\"## Even more customization by User Defined Function\n\nNot tensorflow related, but useful to mention.\n\"\"\"\n\ndef u_mul_e_udf(edges):\n  return {'m' : edges.src['h'] * edges.data['w']}\n\ndef mean_udf(nodes):\n  return {'h_N': nodes.mailbox['m'].mean(1)}\n\n"]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}